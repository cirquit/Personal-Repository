{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of MNIST digits from sound\n",
    "\n",
    "##### Data information:\n",
    "\n",
    "We have a dataset of `8800` spoken digits, encoded as a time series of `13` Mel-frequency cepstral coefficients. \n",
    "\n",
    "\n",
    "###### Train Set:\n",
    "\n",
    "Filename: `Train_Arabic_Digits.txt`\n",
    "\n",
    "Each block delimited by an empty line corresponds to a one analysis frame which consist of `4-93` lines of `13 MEL` coefficients.\n",
    "\n",
    "The first `330` blocks correspond to male speakers, the next `330` are spoken by females.\n",
    "\n",
    "Block `1-660` correspond to the digit `0`, `661-1320` to the digit `1`, etc.\n",
    "\n",
    "In summary we should have `6600` test data samples.\n",
    "\n",
    "###### Test Set:\n",
    "\n",
    "Filename: `Test_Arabic_Digits.txt`\n",
    "\n",
    "Same as the training set, except we have only `220` for each digit, the first `110` are spoken by males, and the other `110` by females.\n",
    "\n",
    "In summary we should have `2200` test data samples.\n",
    "\n",
    "\n",
    "Link to the [data source](http://archive.ics.uci.edu/ml/datasets/Spoken+Arabic+Digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import operator\n",
    "import functools\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index to track how many samples we've seen for the current class\n",
    "s_ix   = 0\n",
    "\n",
    "trainings_set_raw = [[]]\n",
    "\n",
    "with open('Train_Arabic_Digits.txt', 'r') as file:\n",
    "    for lineix, line in enumerate(file):\n",
    "        if line.isspace():\n",
    "            s_ix += 1\n",
    "            trainings_set_raw.append([])\n",
    "        else:\n",
    "            coeffs = [float(s) for s in line.split(' ')]\n",
    "            trainings_set_raw[s_ix].append(coeffs)\n",
    "\n",
    "s_ix   = 0\n",
    "\n",
    "test_set_raw = [[]]\n",
    "\n",
    "with open('Test_Arabic_Digits.txt', 'r') as file:\n",
    "    for lineix, line in enumerate(file):\n",
    "        if line.isspace():\n",
    "            s_ix += 1\n",
    "            test_set_raw.append([])\n",
    "        else:\n",
    "            coeffs = [float(s) for s in line.split(' ')]\n",
    "            test_set_raw[s_ix].append(coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest timestep size: 4 for 6600 samples\n",
      "Greatest timestep size: 93 for 6600 samples\n",
      "Smallest timestep size: 7 for 6600 samples\n",
      "Greatest timestep size: 83 for 6600 samples\n"
     ]
    }
   ],
   "source": [
    "# as the samples have different length, we will clip them to the smallest size\n",
    "\n",
    "def check_timestep_size(dataset):\n",
    "    min_len = sys.maxsize\n",
    "    max_len = 0\n",
    "    for sample in range(1,len(dataset)):\n",
    "        cur_len = len(dataset[sample])\n",
    "        if  cur_len < min_len:\n",
    "            min_len = cur_len\n",
    "        if  cur_len > max_len:\n",
    "            max_len = cur_len\n",
    "\n",
    "    print('Smallest timestep size: {0} for {1} samples'.format(min_len, len(trainings_set)))\n",
    "    print('Greatest timestep size: {0} for {1} samples'.format(max_len, len(trainings_set)))\n",
    "    \n",
    "check_timestep_size(trainings_set_raw)\n",
    "check_timestep_size(test_set_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest timestep size: 93 for 6600 samples\n",
      "Greatest timestep size: 93 for 6600 samples\n",
      "Smallest timestep size: 93 for 6600 samples\n",
      "Greatest timestep size: 93 for 6600 samples\n"
     ]
    }
   ],
   "source": [
    "# we will append zeros to every sample until it each one reaches 93 timesteps\n",
    "\n",
    "def add_zero_padding(dataset):\n",
    "    \n",
    "    zero_padding = [0.] * 13\n",
    "    \n",
    "    for ix in range(1,len(dataset)):\n",
    "        while len(dataset[ix]) < 93:\n",
    "            dataset[ix].append(zero_padding)\n",
    "\n",
    "add_zero_padding(trainings_set_raw)\n",
    "add_zero_padding(test_set_raw)\n",
    "\n",
    "# let's check the sizes again\n",
    "check_timestep_size(trainings_set_raw)\n",
    "check_timestep_size(test_set_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is flattened: True\n",
      "Everything is flattened: True\n"
     ]
    }
   ],
   "source": [
    "# now we will flatten the timesteps so every sample will have a dimension of 93*13\n",
    "\n",
    "def flatten_set(dataset):\n",
    "    is_flattened = True\n",
    "    new_dataset  = [0] * len(dataset)\n",
    "    \n",
    "    for ix in range(1, len(dataset)):\n",
    "        new_dataset[ix] = functools.reduce(operator.add, dataset[ix])\n",
    "        is_flattened &= len(new_dataset[ix]) == 93 * 13\n",
    "            \n",
    "    print('Everything is flattened: {0}'.format(is_flattened))\n",
    "\n",
    "trainings_set = flatten_set(trainings_set_raw)\n",
    "test_set      = flatten_set(test_set_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we have the create the target labels\n",
    "\n",
    "trainings_set_targets = [ [0] * 660\n",
    "                        , [1] * 660\n",
    "                        , [2] * 660\n",
    "                        , [3] * 660\n",
    "                        , [4] * 660\n",
    "                        , [5] * 660\n",
    "                        , [6] * 660\n",
    "                        , [7] * 660\n",
    "                        , [8] * 660\n",
    "                        , [9] * 660]\n",
    "\n",
    "test_set_targets = [ [0] * 220\n",
    "                   , [1] * 220\n",
    "                   , [2] * 220\n",
    "                   , [3] * 220\n",
    "                   , [4] * 220\n",
    "                   , [5] * 220\n",
    "                   , [6] * 220\n",
    "                   , [7] * 220\n",
    "                   , [8] * 220\n",
    "                   , [9] * 220]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mnist-sounds]",
   "language": "python",
   "name": "conda-env-mnist-sounds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
